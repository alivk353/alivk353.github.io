# Scrapy

## Base

```bash
scrapy startproject scrapy_spiders
```

- scrapy.cfg: 项目的配置文件
- scrapy_spiders/: 该项目的python模块。之后您将在此加入代码。
- scrapy_spiders/items.py: 项目中的item文件.
- scrapy_spiders/pipelines.py: 项目中的pipelines文件.
- scrapy_spiders/settings.py: 项目的设置文件.
- scrapy_spiders/spiders/: 放置spider代码的目录.



## 关于scrapy的性能瓶颈

### CONCURRENT_REQUESTS 并发数

- CONCURRENT_REQUESTS_PER_DOMAIN
  - 无论任何时候，并发数都不能超过CONCURRENT_REQUESTS

- CONCURRENT_REQUESTS_PER_IP
  - 如果你设置了此项，CONCURRENT_REQUESTS_PER_DOMAIN就会被忽略，并发请求数就是每个IP的请求数量。对于共享站点，比如，多个域名指向一个服务器，这可以帮助你降低服务器的压力。